print(f"next_only: 목표({target}) 미도달 - 현재 {cur}")
return False
try:
    from playwright_stealth import Stealth
except ImportError:
    Stealth = None
from collections import Counter
from bs4 import BeautifulSoup
from openpyxl import load_workbook
import pandas as pd
import random
import time
import shutil
import re
import os
import json
import requests
import sys
from urllib.parse import urlsplit, urlunsplit, parse_qsl, urlencode


# ?섏씠吏??理쒕? ?щ·留??곹뭹 ??(0?대㈃ ?쒗븳 ?놁쓬)
MAX_PRODUCTS_PER_PAGE = int(os.getenv("MAX_PRODUCTS_PER_PAGE", "0") or 0)


# .env 濡쒕뜑 (python-dotenv 誘몄꽕移???理쒖냼 ?뚯꽌)
try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None


def fallback_load_dotenv(dotenv_path):
    path = Path(dotenv_path)
    try:
        lines = path.read_text(encoding="utf-8").splitlines()
    except FileNotFoundError:
        return False
    except OSError as exc:
        print(f".env ?뚯씪???쎈뒗 以??ㅻ쪟媛 諛쒖깮?덉뒿?덈떎: {exc}")
        return False

    loaded = False
    saw_assignments = False
    for raw_line in lines:
        line = raw_line.strip()
        if not line or line.startswith("#"):
            continue
        if "=" not in line:
            continue
        key, value = line.split("=", 1)
        saw_assignments = True
        key = key.strip()
        value = value.strip().strip("\"'")
        if not key:
            continue
        if key not in os.environ:
            os.environ[key] = value
            loaded = True
    return loaded or saw_assignments


SCRIPT_DIR = Path(__file__).resolve().parent
LOG_FILE = SCRIPT_DIR / "log.txt"


def resolve_category_path() -> Path:
    # ?곗꽑 濡쒖뺄 寃쎈줈, ?놁쑝硫??곸쐞 ?붾젆?곕━(AGENTS.md 媛?대뱶??留욎땄)
    local = SCRIPT_DIR / "naver_category.xlsx"
    parent = SCRIPT_DIR.parent / "naver_category.xlsx"
    if local.exists():
        return local
    if parent.exists():
        return parent
    # 留덉?留됱쑝濡?濡쒖뺄 寃쎈줈瑜?諛섑솚(?ㅽ뙣 ???고??꾩뿉???먮윭 硫붿떆吏 ?쒓났)
    return local


NAVER_CATEGORY_PATH = resolve_category_path()
CRAWLER_DRY_RUN = os.getenv("CRAWLER_DRY_RUN", "0").lower() in {"1", "true", "yes"}

# ?좏깮 ?섏씠吏留??щ·留곹븯???붾쾭洹몄슜 ?듭뀡(?? CRAWL_ONLY_PAGES="51,59").
# 吏?뺣릺吏 ?딆쑝硫?湲곗〈 踰붿쐞(global_start_page~global_last_page) ?꾩껜瑜?泥섎━?⑸땲??
_only_pages_raw = os.getenv("CRAWL_ONLY_PAGES", "").strip()
CRAWL_ONLY_PAGES = None
if _only_pages_raw:
    try:
        parts = re.split(r"[\s,;]+", _only_pages_raw)
        CRAWL_ONLY_PAGES = [int(p) for p in parts if p]
    except Exception as exc:
        print(f"CRAWL_ONLY_PAGES ?뚯떛 ?ㅽ뙣({_only_pages_raw}): {exc}")
        CRAWL_ONLY_PAGES = None

# ?섏씠吏 踰덊샇 ?대룞 ??URL 荑쇰━ ?뚮씪誘명꽣濡?媛뺤젣 ?먰봽 ?쒕룄 ?щ?
# 湲곕낯媛믪? 鍮꾪솢?깊솕(?ㅼ씠踰꾨뒗 URL ?뚮씪誘명꽣留뚯쑝濡?DOM??諛붾뚯? ?딅뒗 寃쎌슦媛 留롮쓬)
PAGE_JUMP_BY_QUERY = os.getenv("PAGE_JUMP_BY_QUERY", "0").lower() in {"1", "true", "yes"}

# ?섏씠吏 ?대룞 怨쇱젙 ?ㅽ겕由곗꺑 ????붾쾭源??⑸룄)
# 湲곕낯媛?鍮꾪솢?깊솕: ?붿껌???곕씪 罹≪쿂 以묐떒
PAGINATION_DEBUG_SHOTS = os.getenv("PAGINATION_DEBUG_SHOTS", "0").lower() in {"1", "true", "yes"}

# ?섏씠吏?ㅼ씠???꾨왂: auto(湲곕낯) | next_only('?ㅼ쓬'留?諛섎났)
PAGINATION_STRATEGY = os.getenv("PAGINATION_STRATEGY", "auto").lower().strip()

def debug_shot(page, label):
    if not PAGINATION_DEBUG_SHOTS:
        return
    try:
        ts = time.strftime("%Y%m%d_%H%M%S")
        dbg = (SCRIPT_DIR / "debug")
        dbg.mkdir(exist_ok=True)
        path = dbg / f"pagination_{ts}_{label}.png"
        page.screenshot(path=str(path), full_page=True)
        print(f"Saved screenshot: {path}")
    except Exception as exc:
        print(f"Failed to take screenshot({label}): {exc}")

STEALTH_HELPER = Stealth() if Stealth is not None else None
if STEALTH_HELPER is None:
    print(
        "playwright_stealth 紐⑤뱢?먯꽌 Stealth ?대옒?ㅻ? 遺덈윭?ㅼ? 紐삵뻽?듬땲?? "
        "?먯? ?뚰뵾 ?ㅽ겕由쏀듃媛 ?곸슜?섏? ?딆쑝??chromium ?섍꼍?먯꽌??異붽? ?먭????꾩슂?⑸땲??"
    )


if load_dotenv:
    load_dotenv()
else:
    _ = fallback_load_dotenv(SCRIPT_DIR / ".env")


def first_available(node, selectors):
    for selector in selectors:
        try:
            element = node.query_selector(selector)
        except PlaywrightTimeoutError:
            continue
        if element:
            return element
    return None


def find_elements(page, selectors):
    for selector in selectors:
        try:
            elements = page.query_selector_all(selector)
        except PlaywrightTimeoutError:
            continue
        if elements:
            print(f"Selector '{selector}' matched {len(elements)} elements.")
            return elements
    return []


def save_debug_snapshot(page, prefix):
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    path = (SCRIPT_DIR / "debug")
    path.mkdir(exist_ok=True)
    out = path / f"{prefix}_{timestamp}.html"
    try:
        out.write_text(page.content(), encoding="utf-8")
        print(f"Saved debug snapshot: {out}")
    except Exception as exc:
        print(f"Failed to save debug snapshot: {exc}")


def extract_price_from_text(raw_text):
    match = re.search(r"([\d,]+)\s*??, raw_text)
    if match:
        return match.group(1).replace("\u200b", "").strip()
    digits = re.sub(r"[^\d]", "", raw_text)
    if not digits:
        return "N/A"
    try:
        return "{:,}".format(int(digits))
    except ValueError:
        return "N/A"


def update_query_params(url, **params):
    parts = urlsplit(url)
    query = dict(parse_qsl(parts.query, keep_blank_values=True))
    query.update({key: value for key, value in params.items() if value is not None})
    new_query = urlencode(query, doseq=True)
    return urlunsplit((parts.scheme, parts.netloc, parts.path, new_query, parts.fragment))


class Tee(object):
    def __init__(self, *files):
        self.files = files

    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()

    def flush(self):
        for f in self.files:
            f.flush()


f = LOG_FILE.open('w', encoding='utf-8')
original = sys.stdout
sys.stdout = Tee(sys.stdout, f)


base_url = "https://smartstore.naver.com"


def product_list_crawl(context, df, read_excel_path, seen_urls):
    page = context.new_page()
    if browser_name == "chromium" and STEALTH_HELPER:
        STEALTH_HELPER.apply_stealth_sync(page)

    raw_url = 'https://smartstore.naver.com/joypapa_/category/ALL?st=RECENT&dt=BIG_IMAGE&size=80'
    original_url = update_query_params(raw_url, page=None)
    page.goto(original_url)
    page.wait_for_load_state("load")
    page.wait_for_load_state("networkidle")

    global_start_page = 51
    global_last_page = 59
    # ?붾쾭洹? ?뱀젙 ?섏씠吏留??붿껌??寃쎌슦 踰붿쐞瑜??대떦 媛믪쑝濡?異뺤냼
    if CRAWL_ONLY_PAGES:
        try:
            global_start_page = min(CRAWL_ONLY_PAGES)
            global_last_page = max(CRAWL_ONLY_PAGES)
        except Exception:
            pass
    shopname = raw_url.split('/')[3]
    shopnumber = raw_url.split('/')[5].split('?')[0]

    home_dir = Path.home()
    output_folder = home_dir / 'Desktop' / 'excel_output'
    output_folder.mkdir(parents=True, exist_ok=True)

    pagination_button_labels = {
        "next": ["?ㅼ쓬", "?ㅼ쓬 ?섏씠吏", "?ㅼ쓬?섏씠吏", ">"],
        "prev": ["?댁쟾", "?댁쟾 ?섏씠吏", "?댁쟾?섏씠吏", "<"]
    }

    # 寃利?紐⑤뱶: ?뱀젙 ?섏씠吏??泥??곹뭹???댁뼱 湲곕? URL/?대쫫 ?뺤씤
    verify_target_page = None
    verify_expected_url = (os.getenv("VERIFY_FIRST_PRODUCT_URL") or "").strip() or None
    verify_expected_name = (os.getenv("VERIFY_FIRST_PRODUCT_NAME") or "").strip() or None
    try:
        verify_target_page = int(os.getenv("VERIFY_TARGET_PAGE", "") or 0) or None
    except Exception:
        verify_target_page = None

    def scroll_to_pagination():
        try:
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        except PlaywrightTimeoutError:
            pass
        time.sleep(0.8)
        # ?섎떒 ?ㅽ겕濡???媛꾨떒 ?ㅽ겕由곗꺑
        debug_shot(page, "scrolled_bottom")

    def wait_pagination_ready(timeout_ms=8000):
        # ?섏씠吏?ㅼ씠??而⑦뀒?대꼫 ?먮뒗 ?レ옄/踰꾪듉???섑????뚭퉴吏 ?湲?
        selectors = [
            "div[data-shp-area='list.pgn'][role='menubar']",
            "div[data-shp-contents-type='pgn'][role='menubar']",
            "div[data-shp-area-id='pgn'][role='menubar']",
        ]
        end = time.time() + (timeout_ms / 1000.0)
        while time.time() < end:
            container = find_pagination_container()
            if container:
                try:
                    if container.query_selector("a[role='menuitem'],a[role='button']"):
                        return True
                except Exception:
                    pass
            time.sleep(0.3)
        return False

    def get_first_list_href():
        try:
            el = page.query_selector("a[href*='/products/']")
            if el:
                href = el.get_attribute("href")
                return href
        except Exception:
            pass
        return None

    def find_pagination_container():
        candidates = [
            "div[data-shp-area='list.pgn'][role='menubar']",
            "div[data-shp-contents-type='pgn'][role='menubar']",
            "div[data-shp-area-id='pgn'][role='menubar']",
            # ?대갚??
            "nav[aria-label*='?섏씠吏']",
            "nav[aria-label*='pagination']",
            "nav[role='navigation']",
            "div[class*='Pagination']",
            "div[class*='paginate']",
            "div[class*='paging']",
        ]
        for sel in candidates:
            try:
                elem = page.query_selector(sel)
            except Exception:
                elem = None
            if elem:
                return elem
        return None

    def get_pgn_from_container():
        """而⑦뀒?대꼫??data-shp-filter_con ?띿꽦?먯꽌 pgn 媛믪쓣 ?뚯떛(?ㅼ씠踰?援ъ“ ?뱁솕)."""
        try:
            container = find_pagination_container()
            if not container:
                return None
            raw = container.get_attribute("data-shp-filter_con")
            if not raw:
                return None
            # HTML ?몄퐫?⑸맂 臾몄옄??泥섎━
            raw = raw.replace('&quot;', '"')
            data = json.loads(raw)
            if isinstance(data, list):
                for item in data:
                    if isinstance(item, dict) and item.get('key') in {'pgn', 'page', 'pageNum'}:
                        val = item.get('value')
                        try:
                            return int(re.findall(r"\d+", str(val))[0])
                        except Exception:
                            pass
        except Exception:
            return None
        return None

    def find_page_link_in_container(target_page):
        container = find_pagination_container()
        if container is None:
            return None
        # ?묎렐????븷 湲곕컲?쇰줈 ?곗꽑 ?먯깋
        try:
            cand = container.get_by_role("link", name=str(target_page))
            if cand and cand.count() > 0:
                return cand.first
        except Exception:
            pass
        try:
            cand = container.get_by_role("button", name=str(target_page))
            if cand and cand.count() > 0:
                return cand.first
        except Exception:
            pass
        # ?レ옄 ?띿뒪???꾪꽣 ?먯깋
        for sel in ["a", "button", "span", "li"]:
            for node in container.query_selector_all(sel):
                try:
                    t = (node.inner_text() or "").strip()
                except Exception:
                    continue
                if t.isdigit() and int(t) == target_page:
                    try:
                        href = node.get_attribute('href') if sel == 'a' else None
                    except Exception:
                        href = None
                    if href and '/products/' in href:
                        # 제품 상세 링크를 페이지 링크로 오탐한 경우 제외
                        continue
                    return node
        return None

    def ensure_group_has_page(target_page, max_group_hops=20):
        """?寃??レ옄 留곹겕媛 ?꾩옱 蹂댁씠??洹몃９???섑??섎룄濡?'?ㅼ쓬' 洹몃９ ?대룞??諛섎났."""
        hops = 0
        while hops < max_group_hops:
            scroll_to_pagination()
            link = find_page_link_in_container(target_page)
            if link:
                return True
            # ?ㅼ쓬 洹몃９ ?대룞 ?쒕룄
            before_sig = get_first_list_href()
            print(f"?寃?{target_page}媛 蹂댁씠吏 ?딆븘 '?ㅼ쓬' 洹몃９ ?대룞 ?쒕룄")
            if not click_pagination_control("next"):
                break
            # 由ъ뒪??蹂???湲?(泥??곹뭹 href 蹂寃?湲곗?)
            for _ in range(10):
                time.sleep(0.5)
                after_sig = get_first_list_href()
                if before_sig and after_sig and before_sig != after_sig:
                    break
            hops += 1
        return False

    def get_current_page_number():
        # 0) 而⑦뀒?대꼫??pgn 媛??뚯떛 ?쒕룄
        pgn_val = get_pgn_from_container()
        if isinstance(pgn_val, int):
            return pgn_val
        selectors = [
            'a[aria-current="true"]',
            'button[aria-current="true"]',
            '[aria-current="page"]'
        ]
        # 而⑦뀒?대꼫?먯꽌 role=menuitem + aria-current ?곗꽑 ?뺤씤
        try:
            container = find_pagination_container()
            if container:
                node = container.query_selector("a[role='menuitem'][aria-current='true']")
                if node:
                    txt = (node.inner_text() or '').strip()
                    m = re.search(r'\d+', txt)
                    if m:
                        return int(m.group(0))
        except Exception:
            pass
        for selector in selectors:
            locator = page.locator(selector)
            try:
                locator.first.wait_for(state="attached", timeout=5000)
            except PlaywrightTimeoutError:
                continue

            try:
                text = locator.first.inner_text().strip()
            except PlaywrightTimeoutError:
                continue

            match = re.search(r'\d+', text)
            if match:
                return int(match.group())
        # 荑쇰━?ㅽ듃留곸뿉??page ?뚮씪誘명꽣 異붿텧 ?쒕룄
        try:
            from urllib.parse import urlsplit, parse_qsl
            parts = urlsplit(page.url)
            query = dict(parse_qsl(parts.query, keep_blank_values=True))
            for key in ["page", "pageIndex", "pagingIndex", "pageNum", "p"]:
                if key in query:
                    try:
                        return int(re.findall(r"\d+", query[key])[0])
                    except Exception:
                        pass
        except Exception:
            pass
        print(f"?꾩옱 ?섏씠吏 踰덊샇 ?먯깋 ?ㅽ뙣 - URL: {page.url}")
        try:
            print("aria-current ?꾨낫:", page.locator('[aria-current]').all_inner_texts())
        except Exception:
            pass
        return None

    def find_page_link(target_page):
        # 1) 湲곗〈 ?ㅻ퉬寃뚯씠??留곹겕(a[role=menuitem])?먯꽌 寃??
        for link in page.query_selector_all('a[role="menuitem"]'):
            try:
                text = link.inner_text().strip()
            except PlaywrightTimeoutError:
                continue
            match = re.search(r'^\d+$', text)
            if match and int(text) == target_page:
                return link

        # 2) ?묎렐????븷 湲곕컲 ?먯깋
        try:
            candidate = page.get_by_role("link", name=str(target_page))
            if candidate and candidate.count() > 0:
                return candidate.first
        except Exception:
            pass
        try:
            candidate = page.get_by_role("button", name=str(target_page))
            if candidate and candidate.count() > 0:
                return candidate.first
        except Exception:
            pass

        # 3) ?ш큵???먯깋(a, button)?먯꽌 ?띿뒪?멸? ?レ옄留뚯씠怨??寃잕낵 ?쇱튂?섎뒗 ?붿냼 ?좏깮
        for sel in ["a", "button"]:
            for link in page.query_selector_all(sel):
                try:
                    text = link.inner_text().strip()
                except Exception:
                    continue
                if not text:
                    continue
                if re.fullmatch(r"\d+", text) and int(text) == target_page:
                    return link

        return None

    def verify_first_product_on_page():
        try:
            # 泥??곹뭹 留곹겕 ?먯깋
            page.wait_for_selector("a[href*='/products/']", timeout=10000)
            first_link = page.query_selector("a[href*='/products/']")
            if not first_link:
                print("寃利??ㅽ뙣: 泥??곹뭹 留곹겕瑜?李얠? 紐삵뻽?듬땲??")
                return False
            href = first_link.get_attribute("href") or ""
            first_link.click()
            page.wait_for_load_state("load")
            try:
                page.wait_for_load_state("networkidle", timeout=10000)
            except PlaywrightTimeoutError:
                pass
            final_url = page.url
            print(f"寃利앹슜 ?대룞 URL: {final_url}")
            ok_url = True
            if verify_expected_url:
                ok_url = (verify_expected_url in final_url)
            ok_name = True
            if verify_expected_name:
                try:
                    content_text = page.inner_text("body")
                except Exception:
                    content_text = ""
                ok_name = (verify_expected_name in content_text)
            if ok_url and ok_name:
                print("寃利??깃났: 湲곕? URL/?대쫫怨??쇱튂?⑸땲??")
                return True
            else:
                print(f"寃利?寃곌낵: URL?쇱튂={ok_url}, ?대쫫?쇱튂={ok_name}")
                return False
        except Exception as exc:
            print(f"寃利?以??덉쇅: {exc}")
            return False

    def click_pagination_control(direction):
        labels = pagination_button_labels[direction]
        for label in labels:
            try:
                page.get_by_role("button", name=label).click(timeout=1500)
                page.wait_for_load_state("networkidle")
                time.sleep(1)
                return True
            except PlaywrightTimeoutError:
                continue
            except Exception:
                continue

        # 而⑦뀒?대꼫 ?대??먯꽌 '?ㅼ쓬/?댁쟾' ?곗꽑 ?먯깋
        container = find_pagination_container()
        if container:
            try:
                # role=button + aria-hidden=false ?꾨낫??寃??
                for node in container.query_selector_all("a[role='button'],button[role='button']"):
                    try:
                        hidden = node.get_attribute('aria-hidden')
                    except Exception:
                        hidden = None
                    if hidden == 'true':
                        continue
                    try:
                        t = (node.inner_text() or '').strip()
                    except Exception:
                        t = ''
                    if direction == 'next' and ("?ㅼ쓬" in t or t in {"??, ">", "쨩"}):
                        node.click()
                        page.wait_for_load_state("networkidle")
                        time.sleep(1)
                        return True
                    if direction == 'prev' and ("?댁쟾" in t or t in {"??, "<", "짬"}):
                        node.click()
                        page.wait_for_load_state("networkidle")
                        time.sleep(1)
                        return True
                # ?띿뒪??湲곕컲 ?꾨낫
                text_sel = (
                    "a:has-text('?ㅼ쓬'),button:has-text('?ㅼ쓬')" if direction == 'next' else "a:has-text('?댁쟾'),button:has-text('?댁쟾')"
                )
                cand = container.query_selector(text_sel)
                if cand:
                    try:
                        cand.scroll_into_view_if_needed()
                    except Exception:
                        pass
                    cand.click()
                    page.wait_for_load_state("networkidle")
                    time.sleep(1)
                    return True
                # 援ъ“ 湲곕컲 ?대갚: role=button ?듭빱 諛곗뿴?????앹쓣 ?ъ슜
                rb = container.query_selector_all("a[role='button'],button[role='button']")
                if rb:
                    try:
                        node = rb[-1] if direction == 'next' else rb[0]
                        try:
                            node.scroll_into_view_if_needed()
                        except Exception:
                            pass
                        node.click()
                        page.wait_for_load_state("networkidle")
                        time.sleep(1)
                        return True
                    except Exception:
                        pass
                # data-shp-contents-id 蹂댁쑀 ?붿냼 ?곗꽑 ?대┃(?ㅼ씠踰??뱁솕)
                rb2 = container.query_selector_all("a[role='button'][data-shp-contents-id],button[role='button'][data-shp-contents-id]")
                if rb2:
                    try:
                        node = rb2[-1] if direction == 'next' else rb2[0]
                        try:
                            node.scroll_into_view_if_needed()
                        except Exception:
                            pass
                        node.click()
                        page.wait_for_load_state("networkidle")
                        time.sleep(1)
                        return True
                    except Exception:
                        pass
            except Exception:
                pass

        # 페이지 전체 레벨 포괄 선택자는 오탐 위험이 높아 사용하지 않음
        return False

    def go_to_page_number(target_page):
        attempt = 0
        max_attempts = 30
        # 상세페이지에 머물러 있는 경우 목록으로 복귀
        try:
            if '/products/' in (page.url or ''):
                print(f"현재 상세페이지 감지, 목록으로 복귀: {page.url} -> {original_url}")
                try:
                    page.goto(original_url)
                    page.wait_for_load_state("networkidle")
                    time.sleep(1)
                except Exception as exc:
                    print(f"목록 복귀 실패: {exc}")
        except Exception:
            pass
        # next_only ?꾨왂: ?レ옄 留곹겕 ?ъ슜 ?놁씠 '?ㅼ쓬'留?諛섎났 ?대┃
        if PAGINATION_STRATEGY == 'next_only':
            scroll_to_pagination()
            wait_pagination_ready(8000)
            cur = get_current_page_number() or 1
            target = int(target_page)
            print(f"next_only: ?꾩옱 {cur} ??紐⑺몴 {target}")
            # ?덉쟾 踰붿쐞 ?댁뿉??紐⑺몴源뚯? ?꾩쭊
            for _ in range(min(200, max(0, target - cur) + 20)):
                if cur >= target:
                    break
                before_sig = get_first_list_href()
                if not click_pagination_control('next'):
                    print("next 踰꾪듉 ?대┃ ?ㅽ뙣")
                    return False
                for __ in range(12):
                    time.sleep(0.5)
                    after_sig = get_first_list_href()
                    if before_sig and after_sig and before_sig != after_sig:
                        break
                # aria-current媛 ?놁쓣 ???덉쑝誘濡?蹂댁닔?곸쑝濡?利앷?
                new_cur = get_current_page_number()
                cur = new_cur if new_cur is not None else (cur + 1)
            if cur == target:
                print(f"next_only: ?섏씠吏 {target} ?꾨떖")
                return True
            print("next_only: 목표 미도달")`r`n            return False
        while attempt < max_attempts:
            attempt += 1
            scroll_to_pagination()
            current_page_num = get_current_page_number()
            debug_shot(page, f"attempt{attempt}_target{target_page}_after_detect")

            if current_page_num == target_page:
                print(f"?섏씠吏 {target_page}???대? ?꾩튂???덉뒿?덈떎.")
                return True

            # 癒쇱? ?꾩옱 蹂댁씠??洹몃９ ?댁뿉???寃??レ옄 留곹겕瑜?李얠쓬
            page_link = find_page_link_in_container(target_page)
            if page_link:
                try:
                    page_link.scroll_into_view_if_needed()
                except PlaywrightTimeoutError:
                    pass
                time.sleep(0.3)
                debug_shot(page, f"attempt{attempt}_target{target_page}_before_link_click")
                before_sig = get_first_list_href()
                page_link.click()
                page.wait_for_load_state("networkidle")
                time.sleep(1)
                debug_shot(page, f"attempt{attempt}_target{target_page}_after_link_click")
                if get_current_page_number() == target_page:
                    print(f"?섏씠吏 {target_page}濡??대룞 ?꾨즺, ?꾩옱 URL: {page.url}")
                    return True
                # ?섏씠吏 踰덊샇 ?먮떒??遺덇???寃쎌슦, 由ъ뒪???쒓렇?덉쿂 蹂寃쎌쑝濡??대룞 寃利?
                after_sig = get_first_list_href()
                if before_sig and after_sig and before_sig != after_sig:
                    print(f"由ъ뒪??蹂寃?媛먯?濡??섏씠吏 {target_page} ?대룞 ?깃났?쇰줈 媛꾩＜")
                    return True
                continue

            if current_page_num is None:
                print("?꾩옱 ?섏씠吏 踰덊샇瑜??뺤씤?????놁뼱 ?ㅼ떆 ?쒕룄?⑸땲??")
                # URL ?뚮씪誘명꽣 湲곕컲 ?먰봽瑜??곗꽑 1???쒕룄
                if PAGE_JUMP_BY_QUERY and attempt in {1, 5, 10, 20}:
                    try:
                        for key in ["page", "pageIndex", "pagingIndex", "pageNum", "p"]:
                            jump_url = update_query_params(original_url, **{key: target_page})
                            print(f"URL ?먰봽 ?쒕룄(踰덊샇 誘명깘吏): {jump_url}")
                            page.goto(jump_url)
                            page.wait_for_load_state("networkidle")
                            time.sleep(1)
                            num = get_current_page_number()
                            if num == target_page:
                                print(f"URL ?먰봽濡??섏씠吏 {target_page} ?대룞 ?뺤씤")
                                return True
                    except Exception as exc:
                        print(f"URL ?먰봽 ?ㅽ뙣: {exc}")
                # ?レ옄 留곹겕媛 蹂댁씠??洹몃９???꾨땺 ???덉쑝??洹몃９ ?대룞 ?쒕룄
                if ensure_group_has_page(target_page):
                    continue
                page.wait_for_timeout(1000)
                continue

            direction = "next" if target_page > current_page_num else "prev"
            print(f"?섏씠吏 {target_page} ?대룞???꾪빐 {direction} 踰꾪듉 ?대┃ ?쒕룄 (?꾩옱 {current_page_num}).")
            before_sig = get_first_list_href()
            if not click_pagination_control(direction):
                print(f"{direction} 踰꾪듉??李얠쓣 ???놁뒿?덈떎.")
                # 踰꾪듉 ?먯깋 ?ㅽ뙣 ??URL ?뚮씪誘명꽣 湲곕컲 ?먰봽 ?쒕룄
                if PAGE_JUMP_BY_QUERY:
                    try:
                        for key in ["page", "pageIndex", "pagingIndex", "pageNum", "p"]:
                            jump_url = update_query_params(original_url, **{key: target_page})
                            print(f"URL ?먰봽 ?쒕룄: {jump_url}")
                            debug_shot(page, f"attempt{attempt}_target{target_page}_before_url_jump")
                            page.goto(jump_url)
                            page.wait_for_load_state("networkidle")
                            time.sleep(1)
                            debug_shot(page, f"attempt{attempt}_target{target_page}_after_url_jump")
                            num = get_current_page_number()
                            if num == target_page:
                                print(f"URL ?먰봽濡??섏씠吏 {target_page} ?대룞 ?뺤씤")
                                return True
                    except Exception as exc:
                        print(f"URL ?먰봽 ?ㅽ뙣: {exc}")
                return False
            # 由ъ뒪??蹂寃쎌쑝濡??대룞 寃利?
            for _ in range(10):
                time.sleep(0.5)
                after_sig = get_first_list_href()
                if before_sig and after_sig and before_sig != after_sig:
                    break

        print(f"?섏씠吏 {target_page} ?대룞 ?쒕룄媛 {max_attempts}??珥덇낵濡??ㅽ뙣?덉뒿?덈떎.")
        return False

    # 吏?뺣맂 ?섏씠吏留??щ·留곹븯?꾨줉 ?쒗븳(?덉쓣 寃쎌슦)
    only_pages_set = set(CRAWL_ONLY_PAGES) if CRAWL_ONLY_PAGES else None

    for start_page in range(global_start_page, global_last_page + 1, 10):
        last_page = min(start_page + 9, global_last_page)

        # ??洹몃９(10?섏씠吏 臾띠쓬)??泥섎━???섏씠吏媛 ?놁쑝硫?嫄대꼫?
        if only_pages_set is not None:
            group_pages = set(range(start_page, last_page + 1))
            group_target_pages = sorted(group_pages & only_pages_set)
            if not group_target_pages:
                print(f"Skip page group {start_page}-{last_page} (no target pages in CRAWL_ONLY_PAGES)")
                continue
        else:
            group_target_pages = list(range(start_page, last_page + 1))
        write_excel_path = output_folder / f'dolce_{shopname}_{shopnumber}_{start_page}_{last_page}.xlsx'
        shutil.copy(read_excel_path, write_excel_path)

        # 洹몃９ ??理쒖큹 ?寃??섏씠吏濡??대룞
        first_target = group_target_pages[0]
        if not go_to_page_number(first_target):
            print(f"?섏씠吏 {start_page} ?대룞???ㅽ뙣?덉뒿?덈떎. ?ㅼ쓬 洹몃９?쇰줈 ?섏뼱媛묐땲??")
            continue

        for page_number in group_target_pages:
            if not go_to_page_number(page_number):
                print(f"?섏씠吏 {page_number} ?대룞???ㅽ뙣?섏뿬 嫄대꼫?곷땲??")
                continue
            # 寃利?紐⑤뱶: ????섏씠吏?먯꽌 泥??곹뭹 ?댁뼱 ?뺤씤 ??醫낅즺
            if verify_target_page and page_number == verify_target_page:
                ok = verify_first_product_on_page()
                print(f"VERIFY_RESULT: page={page_number}, ok={ok}")
                return
            df, _ = crawl_page(page, df, seen_urls)
            print(f"Completed page {page_number}")

        write_to_excel(df, write_excel_path, seen_urls)
        write_to_excel2(
            df,
            output_folder / f'dolce_{shopname}_{shopnumber}_{start_page}_{last_page}_second.xlsx'
        )
        print(f"Processed pages {start_page} to {last_page}")

    page.close()


def find_content_element(page, product_code):
    time.sleep(1)

    content_selectors = [
        '#INTRODUCE .detail_viewer',
        '#INTRODUCE [data-component-id]',
        '#INTRODUCE',
        '[data-name="INTRODUCE"][role="tabpanel"]',
        'xpath=//*[@id="INTRODUCE"]//div[contains(@data-component-id,"INTRODUCE")]//div[contains(@class,"se_component")]//div[last()]',
        'xpath=//*[@id="INTRODUCE"]/div/div[4]',
    ]

    for selector in content_selectors:
        element = page.query_selector(selector)
        if element is not None:
            print(f"Using content selector '{selector}' for {product_code}")
            return selector

    print("?곹뭹 ?곸꽭 而⑦뀗痢??곸뿭??李얠? 紐삵뻽?듬땲?? ?ㅻ깄???????None 諛섑솚")
    save_debug_snapshot(page, f"content_{product_code}")
    return None


def crawl_page(page, df, seen_urls):
    time.sleep(1)
    page.wait_for_load_state("networkidle")
    # ?곹뭹 留곹겕 ?깆옣 ?湲?(?숈쟻 濡쒕뵫 ?鍮?
    try:
        page.wait_for_selector("a[href*='/products/']", timeout=10000)
    except Exception:
        pass
    products = find_elements(
        page,
        [
            "[data-testid='PRODUCT_CARD']",
            "li:has(a[href*='/products/'])",
            "div:has(a[href*='/products/'])",
            "li[class*='flu7YgFW2k']",
        ],
    )
    if not products:
        print("?곹뭹 由ъ뒪????됲꽣媛 紐⑤몢 ?ㅽ뙣?덉뒿?덈떎. HTML ?ㅻ깄?룹쓣 ??ν빀?덈떎.")
        save_debug_snapshot(page, "product_list")
    duplicate_detected = False

    for i, product in enumerate(products):
        product_data = get_product_data(page, product, i, len(products))
        if product_data is None:
            print(f"Skipping product at index {i} as get_product_data returned None.")
            continue

        product_url = product_data['Product_URL'][0]

        if product_url == "N/A" or not product_url:
            print("?곹뭹 URL 異붿텧 ?ㅽ뙣濡???ぉ??嫄대꼫?곷땲??")
            continue

        if product_url in seen_urls:
            print('Duplicate product detected: ', product_url)
            duplicate_detected = True
            break
        else:
            seen_urls.add(product_url)

        df = pd.concat([df, product_data], ignore_index=True)

        if MAX_PRODUCTS_PER_PAGE and (i + 1) >= MAX_PRODUCTS_PER_PAGE:
            print(f"Reached MAX_PRODUCTS_PER_PAGE={MAX_PRODUCTS_PER_PAGE}, stop crawling this page.")
            break

    return df, duplicate_detected


def extract_product_details(product):
    title_element = first_available(
        product,
        [
            "strong[aria-hidden='false']",
            "[data-testid='PRODUCT_CARD_TITLE']",
            "a[href*='/products/'] strong",
            "span[class*='ProductCard__Title']",
            "strong._26YxgX-Nu5",
        ],
    )
    if title_element:
        title = title_element.inner_text().strip()
    else:
        title = product.inner_text().splitlines()[0].strip() if product.inner_text() else "N/A"

    price_element = first_available(
        product,
        [
            "[data-testid='PRODUCT_CARD_PRICE']",
            "span:has-text('??)",
            "strong span:has-text('??)",
            "span._2DywKu0J_8",
        ],
    )
    if price_element:
        price = extract_price_from_text(price_element.inner_text())
    else:
        price = extract_price_from_text(product.inner_text())

    url_element = first_available(
        product,
        [
            "a[href*='/products/'][role='link']",
            "a[href*='/products/']",
            "a._2id8yXpK_k",
        ],
    )

    if url_element:
        raw_url = url_element.get_attribute("href")
        if raw_url and raw_url.startswith("/"):
            product_url = base_url + raw_url
        else:
            product_url = raw_url
    else:
        product_url = None

    product_code = product_url.split('/')[-1] if product_url else "N/A"
    return title, price, product_url or "N/A", product_code


def original_shipping_fee(page):
    print(f"Current page URL: {page.url}")

    shipping_selectors = [
        "xpath=//*[contains(@class,'delivery') and contains(text(),'??)]",
        "xpath=//span[contains(text(),'諛곗넚鍮?)]/following-sibling::*[1]",
        "xpath=//*[contains(text(),'諛곗넚鍮?) and contains(text(),'??)]",
        "xpath=//*[contains(text(),'諛섑뭹諛곗넚鍮?) and contains(text(),'??)]",
    ]

    for selector in shipping_selectors:
        element = page.query_selector(selector)
        if not element:
            continue
        element_text = element.inner_text().strip()
        if "臾대즺諛곗넚" in element_text:
            print("諛곗넚鍮? 臾대즺諛곗넚")
            return "0"
        digits = re.findall(r"[\d,]+", element_text)
        if digits:
            value = digits[0].replace(",", "")
            print(f"諛곗넚鍮? {value}")
            return value

    body_text = ""
    try:
        body_text = page.inner_text("body")
    except Exception:
        pass

    if "臾대즺諛곗넚" in body_text:
        print("諛곗넚鍮? 臾대즺諛곗넚(蹂몃Ц ?먯?)")
        return "0"

    for pattern in [r"諛곗넚鍮?s*[:竊??\s*([\d,]+)\s*??, r"諛섑뭹諛곗넚鍮?s*[:竊??\s*([\d,]+)\s*??]:
        match = re.search(pattern, body_text)
        if match:
            value = match.group(1).replace(",", "")
            print(f"諛곗넚鍮?蹂몃Ц ?먯?): {value}")
            return value

    print("Shipping fee element not found, ?????N/A 諛섑솚")
    save_debug_snapshot(page, "shipping_fee")
    return "N/A"


def option_crawl(page):
    option_data = {}

    option_triggers = page.query_selector_all('[data-shp-area$="optselect"]')
    if not option_triggers:
        option_triggers = page.query_selector_all(
            'a[role="button"][aria-haspopup="listbox"], button[aria-haspopup="listbox"]'
        )
        if option_triggers:
            print("Fallback option selector ?ъ슜 (listbox 踰꾪듉 湲곕컲)")

    option_index = 0
    while True:
        current_triggers = page.query_selector_all('[data-shp-area$="optselect"]')
        if not current_triggers:
            current_triggers = option_triggers

        if option_index >= len(current_triggers):
            break

        trigger = current_triggers[option_index]
        data_area = (trigger.get_attribute("data-shp-area") or "")
        if data_area and "optselect" not in data_area:
            option_index += 1
            continue

        option_index += 1
        category = trigger.get_attribute("aria-label") or trigger.inner_text().strip()
        if not category or category in {"?좏깮", ""}:
            category = f"?듭뀡{option_index}"

        try:
            trigger.click()
        except PlaywrightTimeoutError:
            print(f"{category} ?대┃ ?ㅽ뙣")
            continue

        try:
            dropdown = page.wait_for_selector("ul[role=\"listbox\"]", timeout=15000)
        except PlaywrightTimeoutError:
            print(f"{category} ?듭뀡 由ъ뒪??濡쒕뱶 ?ㅽ뙣")
            continue

        items = dropdown.query_selector_all("[role='option'], a, li")
        current_options = []
        current_prices = []

        for item in items:
            option_text = item.inner_text().strip()
            if not option_text:
                continue
            price_match = re.search(r'\(([+\-]?[\d,]+)??)', option_text)
            if price_match:
                price_value = int(price_match.group(1).replace(',', ''))
                name = re.sub(r'\(([+\-]?[\d,]+)??)', '', option_text).strip()
            else:
                price_value = 0
                name = option_text

            current_options.append(name)
            current_prices.append(price_value)

        if not current_options:
            print(f"{category} ?듭뀡 ?뺣낫瑜?李얠? 紐삵뻽?듬땲??")
            continue

        option_data[category] = {
            '?섏쐞?듭뀡?쒕ぉ': current_options,
            '?섏쐞?듭뀡媛寃?: current_prices,
        }

        selectable = []
        for item in items:
            try:
                disabled = item.evaluate("node => node.getAttribute('aria-disabled') === 'true'")
            except Exception:
                disabled = False
            if not disabled:
                selectable.append(item)

        if selectable:
            random.choice(selectable).click()
            time.sleep(0.5)

    return option_data


def image_crawl(page):
    main_candidates = find_elements(
        page,
        [
            "img[alt='??쒖씠誘몄?']",
            "img[alt*='???][src*='shop-phinf']",
            "div[id='content'] img[src*='shop-phinf']",
        ],
    )
    thumbnail_elements = find_elements(
        page,
        [
            "img[alt^='異붽??대?吏']",
            "button[aria-label^='?몃꽕??] img",
            "ul[class*='thumbnail'] img",
        ],
    )

    if not thumbnail_elements:
        thumbnail_elements = page.query_selector_all("img[src*='shop-phinf']")

    image_elements = []
    if main_candidates:
        image_elements.extend(main_candidates)
    if thumbnail_elements:
        image_elements.extend(thumbnail_elements)

    if not image_elements:
        try:
            fallback = page.wait_for_selector(
                'xpath=//*[@id="content"]//img[contains(@src,"shop-phinf")]',
                timeout=5000,
            )
            if fallback:
                image_elements = [fallback]
        except Exception:
            pass

    if not image_elements:
        print("No images found on the page.")
        return [], []

    thumbnail_urls = []
    for element in image_elements:
        src = element.get_attribute("src")
        if not src:
            continue
        thumbnail_urls.append(src.split("?")[0])

    seen = set()
    unique_urls = []
    for url in thumbnail_urls:
        if url in seen:
            continue
        seen.add(url)
        unique_urls.append(url)

    if not unique_urls:
        print("Image URLs could not be extracted.")
        return [], []

    def url_prefix(u):
        filename = u.split("/")[-1]
        digits = "".join(ch for ch in filename if ch.isdigit())
        return digits[:3] if digits else filename[:3]

    prefixes = [url_prefix(url) for url in unique_urls]
    counts = Counter(prefixes)
    if not counts:
        return unique_urls, []

    most_common = counts.most_common(1)[0][0]
    common_urls = [url for url, prefix in zip(unique_urls, prefixes) if prefix == most_common]
    different_urls = [url for url, prefix in zip(unique_urls, prefixes) if prefix != most_common]

    return common_urls, different_urls


def content_crawl(page, product_code, element_selector):
    time.sleep(1)
    page.wait_for_load_state("load")

    if not element_selector:
        print("Invalid element selector. Moving to the next item.")
        return None

    element = page.query_selector(element_selector)
    if element is None:
        print("No valid element found for the selector. Moving to the next item.")
        return None

    content = element.inner_html()
    soup = BeautifulSoup(content, 'html.parser')

    if '怨꾩냽?⑸땲?? in soup.get_text():
        print("found 怨꾩냽?⑸땲??)
        return None

    css_link = soup.new_tag("link", rel="stylesheet",
                            href="https://static-resource-smartstore.pstatic.net/smartstore/p/static/20230630180923/common.css")
    if soup.head:
        soup.head.append(css_link)
    else:
        head_tag = soup.new_tag("head")
        head_tag.append(css_link)
        soup.insert(0, head_tag)

    for button in soup.find_all('button'):
        button.decompose()

    for img in soup.find_all('img', attrs={'data-src': True}):
        img['src'] = img['data-src']
        del img['data-src']

    target_url = "https://rapid-up.s3.ap-northeast-2.amazonaws.com/dev/gray-line.png"
    new_url = "https://axh2eqadoldy.compat.objectstorage.ap-chuncheon-1.oraclecloud.com/bucket-20230610-0005/upload/gray-line.png"
    for img in soup.find_all('img', src=target_url):
        img['src'] = new_url

    text_to_remove = '* {text-align: center;}  #mycontents11 img{max-width: 100%;}'
    if text_to_remove in soup.get_text():
        soup = BeautifulSoup(str(soup).replace(text_to_remove, ''), 'html.parser')

    disallowed_attrs = ['area-hidden', 'data-linkdata', 'data-linktype', 'onclick', 'style', 'class']
    for attr in disallowed_attrs:
        for tag in soup.find_all(attrs={attr: True}):
            del tag[attr]

    for a in soup.find_all('a', attrs={'data-linkdata': True}):
        img = a.find('img')
        if img:
            src = img.get('src', '')
            data_src = img.get('data-src', '')
            if not src:
                src = data_src
                img['src'] = src
                if 'data-src' in img.attrs:
                    del img['data-src']

    for img in soup.find_all('img'):
        src = img.get('src', '')
        if src.startswith(('https://rapid-up.s3.ap-northeast-2.amazonaws.com', 'https://cdn.heyseller.kr', 'https://ai.esmplus.com/')):
            img.decompose()

    print(f"Number of images after all removals: {len(soup.find_all('img'))}")

    soup = insert_and_remove_images(soup)

    for img_tag in soup.find_all('img'):
        img_tag['style'] = 'display: block; margin-left: auto; margin-right: auto; margin-bottom: 10px;'

    for h1 in soup.find_all('h1'):
        h1['style'] = 'text-align: center; font-size: 30px; margin-bottom: 20px;'

    text_elements = ['p', 'div', 'span', 'li', 'a']
    for tag_name in text_elements:
        for element in soup.find_all(tag_name):
            existing_style = element.get('style', '')
            new_style = f"{existing_style}; text-align: center; font-size: 18px; margin-bottom: 30px;"
            element['style'] = new_style.strip()

    return pd.DataFrame({'Content': [str(soup)]})


def insert_and_remove_images(soup):
    img_srcs_to_insert = ['https://axh2eqadoldy.compat.objectstorage.ap-chuncheon-1.oraclecloud.com/bucket-20230610-0005/upload/top.png',
                          'https://axh2eqadoldy.compat.objectstorage.ap-chuncheon-1.oraclecloud.com/bucket-20230610-0005/upload/bottom.png',
                          'https://coudae.s3.ap-northeast-2.amazonaws.com/A00412936/cloud/7290.png']

    img_tag_top = soup.new_tag('img', src=img_srcs_to_insert[0],
                               style="display: block; margin-left: auto; margin-right: auto;")
    img_tag_middle = soup.new_tag('img', src=img_srcs_to_insert[2],
                                  style="display: block; margin-left: auto; margin-right: auto;")
    img_tag_bottom = soup.new_tag('img', src=img_srcs_to_insert[1],
                                  style="display: block; margin-left: auto; margin-right: auto;")

    first_tag = next(soup.children)
    last_tag = next(reversed(soup.contents))

    first_tag.insert_before(img_tag_top)
    last_tag.insert_after(img_tag_bottom)

    img_srcs_to_remove = ['', '']

    for img_src in img_srcs_to_remove:
        img_to_remove = soup.find_all('img', attrs={'src': img_src})
        for img in img_to_remove:
            img.decompose()

    return soup


def return_shipping_fee(total_price):
    fee = total_price * 0.25
    if fee > 200000:
        fee = 200000
    return fee


def title_edit(title):
    title_split = title.split(' ')
    title_split = list(dict.fromkeys(title_split))
    if len(title_split) >= 2:
        title_split[-1], title_split[-2] = title_split[-2], title_split[-1]
    title = ' '.join(title_split)
    return title


def get_product_data(page, product, i, num_products):
    title, price, product_url, product_code = extract_product_details(product)

    title = title.replace('\xa0', ' ')

    print(f"Product {i + 1}/{num_products}: {title}, {price} won, {product_url}")

    product_page = context.new_page()
    if browser_name == "chromium" and STEALTH_HELPER:
        STEALTH_HELPER.apply_stealth_sync(product_page)
    product_page.goto(product_url)
    product_page.wait_for_load_state("load")
    try:
        product_page.wait_for_load_state("networkidle", timeout=10000)
    except PlaywrightTimeoutError:
        pass

    scripts = product_page.query_selector_all('script')

    if not NAVER_CATEGORY_PATH.exists():
        raise FileNotFoundError(f"移댄뀒怨좊━ ?뚯씪??李얠쓣 ???놁뒿?덈떎: {NAVER_CATEGORY_PATH}")
    category_df = pd.read_excel(NAVER_CATEGORY_PATH, header=None)
    small_category_dict = pd.Series(category_df[0].values, index=category_df[3]).to_dict()
    tiny_category_dict = pd.Series(category_df[0].values, index=category_df[4]).to_dict()

    category = None
    for script in scripts:
        script_content = script.inner_text()
        if "category" in script_content:
            try:
                json_data = json.loads(script_content)
            except Exception:
                continue
            if 'category' in json_data:
                category = json_data['category']
                break

    if category is not None:
        print(f"Category: {category}")
        category_list = category.split(">")
        large_category = category_list[0].strip()
        medium_category = category_list[1].strip()
        small_category = category_list[2].strip() if len(category_list) > 2 else None
        tiny_category = category_list[3].strip() if len(category_list) > 3 else None
    else:
        category_list = []
        large_category = None
        medium_category = None
        small_category = None
        tiny_category = None

    if tiny_category is not None:
        smallest_category = tiny_category
        smallest_category_type = 'tiny'
        naver_category_number = tiny_category_dict.get(tiny_category)
    elif small_category is not None:
        smallest_category = small_category
        smallest_category_type = 'small'
        naver_category_number = small_category_dict.get(small_category)
    else:
        smallest_category = None
        smallest_category_type = None
        naver_category_number = None

    print(
        f"Smallest Category('{smallest_category_type}') : {smallest_category}, Naver category number: {naver_category_number}")

    options = option_crawl(product_page)
    print("Options:", options)

    common_urls, different_urls = image_crawl(product_page)
    print(f"common_urls: {common_urls}")

    main_image = None
    other_images = []

    if common_urls:
        main_image = common_urls[0].replace('?type=m510', '')
        other_images = [url.replace('?type=m510', '') for url in common_urls[1:]]
        print(f"other_images: {other_images}")
    else:
        print("No common images found")
        try:
            image_element = page.wait_for_selector('xpath=//*[@id="content"]/div/div[2]/div[1]/div[1]/div[1]/img', timeout=2000)
            image_url = image_element.get_attribute("src")
            main_image = image_url.replace('?type=m510', '')
        except Exception:
            print("No main image found")

    print("Main image:", main_image)
    print("Other images:", other_images)

    print("URLs not starting with most common three digits:")
    for url in different_urls:
        print(url)

    element_selector = find_content_element(product_page, product_code)
    content = content_crawl(product_page, product_code, element_selector)
    shipping_fee = original_shipping_fee(product_page)

    def to_int(value):
        if value in (None, "N/A"):
            return 0
        digits = re.sub(r"[^\d]", "", str(value))
        return int(digits) if digits else 0

    shipping_fee_int = to_int(shipping_fee)
    price_int = to_int(price)
    total_price = price_int + shipping_fee_int

    if isinstance(content, str):
        content_df = pd.DataFrame({'Content': [content]})
    else:
        content_df = content

    product_df = pd.DataFrame({
        'Product': [title],
        'Price': [price],
        'Shipping_Fee': [shipping_fee_int],
        'Total_Price': [total_price],
        'Main_Image': [main_image],
        'Other_Images': [other_images],
        'Options': [options],
        'Product_URL': [product_url],
        'Naver_Category_Number': [naver_category_number]
    })

    product_df = pd.concat([product_df, content_df], axis=1)
    product_page.close()

    return product_df


def write_to_excel(df, excel_path, seen_urls):
    book = load_workbook(excel_path)
    sheet = book['?쇨큵?깅줉']

    b_start_row = c_start_row = e_start_row = h_start_row = ad_start_row = r_start_row = s_start_row = t_start_row = i_start_row = 3
    ap_start_row = aq_start_row = 3

    # ?곗씠?곌? ?놁쑝硫??쒗뵆由용쭔 ??ν븯怨?議곗슜??諛섑솚
    if df is None or len(df) == 0:
        print("DataFrame??鍮꾩뼱 ?덉뼱 ?묒? 湲곕줉???앸왂?⑸땲??")
        book.save(excel_path)
        if os.name != "nt":
            print(f"Excel file saved to {excel_path}. (empty dataset)")
        return

    for i, item in enumerate(df['Naver_Category_Number'], start=b_start_row):
        sheet['B' + str(i)] = item
    for j, item in enumerate(df['Product'], start=c_start_row):
        sheet['C' + str(j)] = item
    for k, (product_price, shipping_fee) in enumerate(zip(df['Price'], df['Shipping_Fee']), start=e_start_row):
        total_price = float(product_price.replace(',', '')) + shipping_fee
        selling_price = total_price - 0.01 * total_price
        selling_price_rounded = round(selling_price / 100.0) * 100.0
        sheet['E' + str(k)] = selling_price_rounded

    for l, _ in enumerate(df.iterrows(), start=h_start_row):
        sheet['H' + str(l)] = "議고빀??
    for m in range(ad_start_row, ad_start_row + len(df)):
        selling_price_rounded = sheet['E' + str(m)].value

        # 諛붿젮留덉폆 遺꾧린
        if selling_price_rounded <= 20000:
            ad_value = 2903608
        elif 20001 <= selling_price_rounded <= 30000:
            ad_value = 2904260
        elif 30001 <= selling_price_rounded <= 40000:
            ad_value = 2904261
        elif 40001 <= selling_price_rounded <= 60000:
            ad_value = 2904262
        elif 60001 <= selling_price_rounded <= 80000:
            ad_value = 2904268
        elif 80001 <= selling_price_rounded <= 100000:
            ad_value = 2904272
        elif 100001 <= selling_price_rounded <= 150000:
            ad_value = 2904276
        elif 150001 <= selling_price_rounded <= 400000:
            ad_value = 2904278
        elif 400001 <= selling_price_rounded <= 600000:
            ad_value = 2904279
        elif 600001 <= selling_price_rounded <= 1000000:
            ad_value = 2904281
        elif 1000001 <= selling_price_rounded <= 9999999:
            ad_value = 2904284

        sheet['AD' + str(m)] = ad_value

    for row, _ in enumerate(df.iterrows(), start=h_start_row):
        sheet['U' + str(row)] = "?곸꽭?섏씠吏 李몄“"
        sheet['V' + str(row)] = "?곸꽭?섏씠吏 李몄“"
        sheet['Y' + str(row)] = "0200037"
        sheet['Z' + str(row)] = "援щℓ???
        sheet['AZ' + str(row)] = "010-3973-3119"
        sheet['BA' + str(row)] = "蹂몃Ц ?덈궡臾?李몄“"

    for l, item in enumerate(df['Options'], start=r_start_row):
        option_titles = []
        option_prices = []
        option_categories = []
        for key in item:
            option_categories.append(key)
            if '?섏쐞?듭뀡?쒕ぉ' in item[key]:
                option_titles.append(', '.join(item[key]['?섏쐞?듭뀡?쒕ぉ']))
            if '?섏쐞?듭뀡媛寃? in item[key]:
                option_prices.append(', '.join(map(str, item[key]['?섏쐞?듭뀡媛寃?])))
        sheet['I' + str(l)] = '\n'.join(option_categories)
        sheet['J' + str(l)] = '\n'.join(option_titles)
        sheet['K' + str(l)] = '\n'.join(option_prices)

    if 'Options' in df.columns:
        for row in range(h_start_row, h_start_row + len(df)):
            item_options = df.at[row - h_start_row, 'Options']
            option_prices = []
            for option in item_options.values():
                if '?섏쐞?듭뀡媛寃? in option:
                    option_prices.extend(option['?섏쐞?듭뀡媛寃?])

            if option_prices:
                num_prices = len(option_prices)
                l_values = ', '.join(['99'] * num_prices)
                sheet['L' + str(row)] = l_values
            else:
                sheet['L' + str(row)] = "99"
    else:
        for row in range(h_start_row, h_start_row + len(df)):
            sheet['L' + str(row)] = "99"

    for n, item in enumerate(df['Main_Image'], start=r_start_row):
        sheet['R' + str(n)] = item
    for o, item in enumerate(df['Other_Images'], start=s_start_row):
        if item is not None:
            sheet['S' + str(o)] = "\n".join(str(img) for img in item if img is not None)
        else:
            sheet['S' + str(o)] = ""

    if 'Content' in df.columns:
        for p, item in enumerate(df['Content'], start=t_start_row):
            if isinstance(item, float):
                item = str(item)
            print(f"Row {p}, Content: {item[:100]}")
            sheet['T' + str(p)] = item
    else:
        print("No 'Content' column found in DataFrame")

    for q, url in enumerate(df['Product_URL'], start=i_start_row):
        product_code = url.split('/')[-1]
        try:
            selling_code = str(int(product_code) * 2)
        except ValueError:
            selling_code = str(random.randint(10000000, 99999999)) + 'R'
        sheet['A' + str(q)] = selling_code
    for r, total_price in enumerate(df['Total_Price'], start=ap_start_row):
        return_fee = return_shipping_fee(total_price)
        return_fee_rounded = round(return_fee / 100.0) * 100
        sheet['AP' + str(r)] = return_fee_rounded
        sheet['AQ' + str(r)] = return_fee_rounded * 2

    book.save(excel_path)

    # ?ㅼそ 鍮????쒓굅
    book = load_workbook(excel_path)
    sheet = book['?쇨큵?깅줉']
    for i in range(3, sheet.max_row + 1):
        if not sheet['A' + str(i)].value:
            sheet.delete_rows(i, sheet.max_row - i + 1)
            break
    book.save(excel_path)

    if os.name == "nt":
        os.system(f'start "" "excel.exe" "{excel_path}"')
    else:
        print(f"Excel file saved to {excel_path}. Automatic Excel launch is skipped on non-Windows platforms.")


def write_to_excel2(df, excel_path2):
    df2 = pd.DataFrame({
        'Product_URL': df['Product_URL'],
        'Numbering': range(1, len(df) + 1),
        'Product_Title': df['Product'],
        'Product_Price': df['Price'],
        'Shipping_Fee': df['Shipping_Fee']
    })
    with pd.ExcelWriter(excel_path2) as writer:
        df2.to_excel(writer, index=False)


# ?ㅽ뻾遺: ??긽 濡쒖뺄 釉뚮씪?곗?瑜??ㅽ뻾 (Windows ?곗꽑)
if CRAWLER_DRY_RUN:
    print("CRAWLER_DRY_RUN=1 ?뚮옒洹몃줈 ?명빐 Playwright ?щ·留?蹂몃룞?묒쓣 ?앸왂?⑸땲??")
    sys.stdout = original
    f.close()
    sys.exit(0)

with sync_playwright() as p:
    browser_name = os.getenv("PLAYWRIGHT_BROWSER", "chromium").lower()
    if browser_name not in {"chromium", "firefox", "webkit"}:
        browser_name = "chromium"

    headless_mode = os.getenv("PLAYWRIGHT_HEADLESS", "0").lower() in {"1", "true", "yes"}

    if browser_name == "chromium":
        launch_args = [
            "--disable-blink-features=AutomationControlled",
            "--disable-features=NetworkService",
            "--disable-web-security",
            "--disable-dev-shm-usage",
            "--disable-accelerated-2d-canvas",
            "--disable-gpu",
        ]
        browser = p.chromium.launch(headless=headless_mode, args=launch_args)
        context = browser.new_context()
    elif browser_name == "firefox":
        browser = p.firefox.launch(headless=headless_mode)
        context = browser.new_context()
    else:
        browser = p.webkit.launch(headless=headless_mode)
        context = browser.new_context()

    if browser_name == "chromium" and STEALTH_HELPER:
        STEALTH_HELPER.apply_stealth_sync(context)

    df = pd.DataFrame(columns=['Product', 'Price', 'Product_URL'])
    output_folder = SCRIPT_DIR / 'output'
    read_excel_path = output_folder / 'ExcelSaveTemplate_230109.xlsx'
    seen_urls = set()

    product_list_crawl(context, df, read_excel_path, seen_urls)
    try:
        context.close()
    finally:
        browser.close()

sys.stdout = original
f.close()



